{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888b6a37",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee5d7c5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Proposition 1 states:\n",
    "$$\n",
    "\\mathbb{E}[R_X(\\hat{\\theta})] = \\frac{n - d}{n} \\sigma^2\n",
    "$$\n",
    "\n",
    "The Bayes risk corresponds to the case where the true parameter $\\theta^*$ is known:\n",
    "$$\n",
    "R_X(\\theta^*) = \\frac{1}{n} \\| y - X\\theta^* \\|^2 = \\frac{1}{n} \\| \\varepsilon \\|^2\n",
    "$$\n",
    "since $y = X\\theta^* + \\varepsilon$. Therefore:\n",
    "$$\n",
    "\\mathbb{E}[R_X(\\theta^*)] = \\frac{1}{n} \\mathbb{E}[\\| \\varepsilon \\|^2]\n",
    "$$\n",
    "\n",
    "Now, because $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$:\n",
    "$$\n",
    "\\mathbb{E}[\\| \\varepsilon \\|^2] = \\mathbb{E}[\\varepsilon^\\top \\varepsilon] = \\mathrm{tr}(\\mathbb{E}[\\varepsilon \\varepsilon^\\top]) = \\mathrm{tr}(\\sigma^2 I_n) = n\\sigma^2\n",
    "$$\n",
    "$$\n",
    "\\Rightarrow \\mathbb{E}[R_X(\\theta^*)] = \\frac{n\\sigma^2}{n} = \\sigma^2\n",
    "$$\n",
    "\n",
    "So:\n",
    "- The Bayes risk is $\\sigma^2$\n",
    "- The expected empirical risk of $\\hat{\\theta}$ is $\\frac{n-d}{n} \\sigma^2$\n",
    "- So we have:\n",
    "$$\n",
    "\\mathbb{E}[R_X(\\hat{\\theta})] < \\mathbb{E}[R_X(\\theta^*)]\n",
    "$$\n",
    "as long as $d > 0$\n",
    "\n",
    "To interpret and discuss this result, we can say that:\n",
    "- The empirical risk evaluated at $\\hat{\\theta}$ is lower than the Bayes risk because $\\hat{\\theta}$ is fitted using the same data it's being evaluated on. This leads to overfitting, which artificially reduces the training error.\n",
    "\n",
    "- As $n \\to \\infty$, the factor $\\frac{n - d}{n} \\to 1$, and the empirical risk of $\\hat{\\theta}$ tends to the Bayes risk.\n",
    "\n",
    "- As $d \\to n$, we have $\\frac{n - d}{n} \\to 0$, which means that the model interpolates the training data almost perfectly, which is a sign of overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8941e0",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eace749",
   "metadata": {},
   "source": [
    "We use the linear model:\n",
    "$$\n",
    "y = X\\theta^* + \\varepsilon\n",
    "$$\n",
    "\n",
    "The OLS estimator is:\n",
    "$$\n",
    "\\hat{\\theta} = (X^\\top X)^{-1} X^\\top y = \\theta^* + (X^\\top X)^{-1} X^\\top \\varepsilon\n",
    "$$\n",
    "\n",
    "Multiplying by $X$:\n",
    "$$\n",
    "X\\hat{\\theta} = X\\theta^* + X(X^\\top X)^{-1} X^\\top \\varepsilon\n",
    "$$\n",
    "\n",
    "Subtracting from $y$:\n",
    "$$\n",
    "y - X\\hat{\\theta} = \\varepsilon - X(X^\\top X)^{-1} X^\\top \\varepsilon = (I_n - H) \\varepsilon\n",
    "$$\n",
    "where $H = X(X^\\top X)^{-1} X^\\top$ is the projection matrix onto $\\mathrm{Im}(X)$.\n",
    "\n",
    "Thus, the empirical risk becomes:\n",
    "$$\n",
    "R_n(\\hat{\\theta}) = \\frac{1}{n} \\| y - X\\hat{\\theta} \\|^2 = \\frac{1}{n} \\| (I_n - H)\\varepsilon \\|^2\n",
    "$$\n",
    "\n",
    "Taking the expectation over $\\varepsilon$:\n",
    "$$\n",
    "\\mathbb{E}[R_n(\\hat{\\theta})] = \\mathbb{E}_\\varepsilon \\left[ \\frac{1}{n} \\| (I_n - H)\\varepsilon \\|^2 \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116382a1",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271be3fb",
   "metadata": {},
   "source": [
    "We know that the entry in row $i$, column $j$ of $A^\\top A$ is:\n",
    "$$\n",
    "(A^\\top A)_{ij} = \\sum_{k=1}^n A_{ki} A_{kj}\n",
    "$$\n",
    "\n",
    "Therefore, the diagonal element $(A^\\top A)_{ii}$ is:\n",
    "$$\n",
    "\\sum_{k=1}^n A_{ki}^2\n",
    "$$\n",
    "\n",
    "So we have:\n",
    "$$\n",
    "\\mathrm{tr}(A^\\top A) = \\sum_{i=1}^n (A^\\top A)_{ii} = \\sum_{i=1}^n \\sum_{k=1}^n A_{ki}^2\n",
    "$$\n",
    "\n",
    "We can rename k to j:\n",
    "$$\n",
    "\\mathrm{tr}(A^\\top A) = \\sum_{i=1}^n \\sum_{j=1}^n A_{ji}^2 = \\sum_{i=1}^n \\sum_{j=1}^n A_{ij}^2\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "$$\n",
    "\\sum_{(i,j) \\in [1, n]^{2}} A_{ij}^2 = \\mathrm{tr}(A^\\top A)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22984446",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f6ca27",
   "metadata": {},
   "source": [
    "Using the euclidean norm:\n",
    "$$\n",
    "\\| A \\varepsilon \\|^2 = (A\\varepsilon)^\\top (A\\varepsilon) = \\varepsilon^\\top A^\\top A \\varepsilon\n",
    "$$\n",
    "\n",
    "We obtain:\n",
    "$$\n",
    "\\mathbb{E}_\\varepsilon \\left[ \\| A \\varepsilon \\|^2 \\right] = \\mathbb{E}_\\varepsilon [\\varepsilon^\\top A^\\top A \\varepsilon]\n",
    "$$\n",
    "\n",
    "We know that for any symmetric matrix $B$ and a Gaussian vector $\\varepsilon \\sim \\mathcal{N}(0, V)$:\n",
    "$$\n",
    "\\mathbb{E}[\\varepsilon^\\top B \\varepsilon] = \\mathrm{tr}(B V)\n",
    "$$\n",
    "\n",
    "In our case:\n",
    "- $B = A^\\top A$\n",
    "- $V = \\sigma^2 I_n$\n",
    "\n",
    "Then:\n",
    "$$\n",
    "\\mathbb{E}[\\varepsilon^\\top A^\\top A \\varepsilon] = \\mathrm{tr}(A^\\top A \\cdot \\sigma^2 I_n) = \\sigma^2 \\mathrm{tr}(A^\\top A)\n",
    "$$\n",
    "\n",
    "So:\n",
    "$$\n",
    "\\mathbb{E}_\\varepsilon \\left[ \\frac{1}{n} \\| A \\varepsilon \\|^2 \\right] = \\frac{\\sigma^2}{n} \\mathrm{tr}(A^\\top A)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a381f1c",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a349d989",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "We are given:\n",
    "$$\n",
    "A = I_n - X(X^\\top X)^{-1}X^\\top = I_n - H\n",
    "$$\n",
    "where $H = X(X^\\top X)^{-1}X^\\top$ is the projection matrix onto the image of $X$.\n",
    "\n",
    "We also know that a projection matrix is symmetric and idempotent, so:\n",
    "- $H^\\top = H$\n",
    "- $H^2 = H$\n",
    "\n",
    "Let's compute $A^\\top$:\n",
    "$$\n",
    "A^\\top = (I_n - H)^\\top = I_n - H^\\top = I_n - H = A\n",
    "$$\n",
    "\n",
    "Let's compute $A^2$:\n",
    "$$\n",
    "A^2 = (I_n - H)^2 = I_n - 2H + H^2 = I_n - 2H + H = I_n - H = A\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "$$\n",
    "A^\\top A = AA = A^2 = A\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bd55c0",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85a2eba",
   "metadata": {},
   "source": [
    "From Question 2:\n",
    "$$\n",
    "\\mathbb{E}[R_n(\\hat{\\theta})] = \\mathbb{E}_\\varepsilon \\left[ \\frac{1}{n} \\| A \\varepsilon \\|^2 \\right]\n",
    "$$\n",
    "\n",
    "From Question 4:\n",
    "$$\n",
    "\\mathbb{E}_\\varepsilon \\left[ \\frac{1}{n} \\| A \\varepsilon \\|^2 \\right] = \\frac{\\sigma^2}{n} \\mathrm{tr}(A^\\top A)\n",
    "$$\n",
    "\n",
    "From Question 5:\n",
    "$$\n",
    "A^\\top A = A \\Rightarrow \\mathrm{tr}(A^\\top A) = \\mathrm{tr}(A)\n",
    "$$\n",
    "\n",
    "Finally, since $A = I_n - H$ and $\\mathrm{tr}(H) = \\mathrm{rank}(X) = d$, we have:\n",
    "$$\n",
    "\\mathrm{tr}(A) = n - d\n",
    "$$\n",
    "\n",
    "So:\n",
    "$$\n",
    "\\mathbb{E}[R_n(\\hat{\\theta})] = \\frac{\\sigma^2}{n} \\mathrm{tr}(A) = \\frac{n - d}{n} \\sigma^2\n",
    "$$\n",
    "\n",
    "which is exactly the result of Proposition 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dc35a8",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7a75e0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The OLS estimator is:\n",
    "$$\n",
    "\\hat{\\theta} = (X^\\top X)^{-1} X^\\top y\n",
    "$$\n",
    "\n",
    "Then:\n",
    "$$\n",
    "X\\hat{\\theta} = X(X^\\top X)^{-1}X^\\top y = Hy\n",
    "$$\n",
    "where $H = X(X^\\top X)^{-1}X^\\top$ is the projection matrix onto $X$.\n",
    "\n",
    "Therefore:\n",
    "$$\n",
    "y - X\\hat{\\theta} = y - Hy = (I_n - H)y\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Since:\n",
    "$$\n",
    "y = X\\theta^* + \\varepsilon\n",
    "$$\n",
    "\n",
    "We can develop:\n",
    "$$\n",
    "(I_n - H)y = (I_n - H)(X\\theta^* + \\varepsilon) = (I_n - H)X\\theta^* + (I_n - H)\\varepsilon\n",
    "$$\n",
    "\n",
    "But, because H is a projection matrix, we know that:\n",
    "$$\n",
    "HX = X\n",
    "$$\n",
    "\n",
    "So:\n",
    "$$\n",
    "(I_n - H)X = X - X =0\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "$$\n",
    "(I_n - H)X\\theta^* + (I_n - H)\\varepsilon = (I_n - H)\\varepsilon\n",
    "$$\n",
    "\n",
    "---\n",
    "We now have:\n",
    "$$\n",
    "\\| y - X\\hat{\\theta} \\|^2 = \\| (I_n - H)\\varepsilon \\|^2\n",
    "$$\n",
    "\n",
    "So:\n",
    "$$\n",
    "\\mathbb{E} \\left[ \\frac{1}{n - d} \\| y - X\\hat{\\theta} \\|^2 \\right]\n",
    "= \\mathbb{E} \\left[ \\frac{1}{n - d} \\| (I_n - H)\\varepsilon \\|^2 \\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Let us denote $A = I_n - H$, as in previous questions.\n",
    "\n",
    "From Question 4, we already showed that:\n",
    "$$\n",
    "\\mathbb{E} \\left[ \\| A\\varepsilon \\|^2 \\right] = \\sigma^2 \\cdot \\mathrm{tr}(A^\\top A)\n",
    "$$\n",
    "\n",
    "And from Question 5, we established that $A$ is symmetric and idempotent, so:\n",
    "$$\n",
    "A^\\top A = A\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "$$\n",
    "\\mathbb{E} \\left[ \\| A\\varepsilon \\|^2 \\right] = \\sigma^2 \\cdot \\mathrm{tr}(A)\n",
    "$$\n",
    "\n",
    "Now compute the trace of $A$:\n",
    "$$\n",
    "A = I_n - H \\Rightarrow \\mathrm{tr}(A) = \\mathrm{tr}(I_n - H) = \\mathrm{tr}(I_n) - \\mathrm{tr}(H) = n - d\n",
    "$$\n",
    "because $\\mathrm{tr}(H) = \\mathrm{rank}(X) = d$.\n",
    "\n",
    "So:\n",
    "$$\n",
    "\\mathbb{E} \\left[ \\| y - X\\hat{\\theta} \\|^2 \\right] = \\mathbb{E} \\left[ \\| (I_n - H)\\varepsilon \\|^2 \\right] = \\mathbb{E} \\left[ \\| A\\varepsilon \\|^2 \\right] = \\sigma^2 (n - d)\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "$$\n",
    "\\mathbb{E} \\left[ \\frac{\\| y - X\\hat{\\theta} \\|^2}{n - d}\\right] = \\frac{1}{n - d}\\mathbb{E} \\left[\\| y - X\\hat{\\theta} \\|^2 \\right] = \\frac{1}{n - d} \\cdot \\sigma^2 (n - d) = \\sigma^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "To conclude, the expected value of: \n",
    "$$\n",
    "\\frac{\\| y - X\\hat{\\theta} \\|^2}{n - d}\n",
    "$$\n",
    "is $\\sigma^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27802131",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f058efbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True sigma^2 = 4.0000\n",
      "Estimated sigma^2 = 3.9480\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(8080)\n",
    "\n",
    "# Parameters\n",
    "n = 10_000\n",
    "d = 10\n",
    "sigma_true = 2\n",
    "sigma2_true = sigma_true ** 2 # = 4\n",
    "\n",
    "X = np.random.randn(n, d)\n",
    "theta_star = np.random.randn(d)\n",
    "epsilon = np.random.normal(loc=0, scale=sigma_true, size=n)\n",
    "y = X @ theta_star + epsilon\n",
    "\n",
    "# Compute OLS estimator\n",
    "XtX_inv = np.linalg.inv(X.T @ X)\n",
    "theta_hat = XtX_inv @ X.T @ y\n",
    "\n",
    "numerator = y - X @ theta_hat\n",
    "\n",
    "# Estimate sigma^2\n",
    "sigma2_hat = (1 / (n - d)) * np.sum(numerator**2)\n",
    "\n",
    "print(f\"True sigma^2 = {sigma2_true:.4f}\")\n",
    "print(f\"Estimated sigma^2 = {sigma2_hat:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b216d4d1",
   "metadata": {},
   "source": [
    "We can see that for a large number of samples the result is indeed consistent with the theoretical value we have chosen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
